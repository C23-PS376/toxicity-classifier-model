{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxyG9UX0100Y"
      },
      "source": [
        "**Training toxicity classifier model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s4Xiwj4N3vJz"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB2-QwlO1_lW"
      },
      "source": [
        "Upload kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaveEDNz4QnO"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Cibrz4P-4gem"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6ryrsSq2GTz"
      },
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9brK8CU5XSD"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c 'jigsaw-toxic-comment-classification-challenge'\n",
        "! unzip 'jigsaw-toxic-comment-classification-challenge.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riFRAi-57MkF"
      },
      "outputs": [],
      "source": [
        "! unzip 'train.csv.zip'\n",
        "! unzip 'test.csv.zip'\n",
        "! unzip 'test_labels.csv.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51WorzPy2Jcs"
      },
      "source": [
        "Download pretrained glove word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdRN0nG5Wig7"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LT0YqglB7TMM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxwA6IFz2Ztr"
      },
      "source": [
        "Load and preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BnbEBLibWgGc"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "df_test_labels = pd.read_csv('test_labels.csv')\n",
        "\n",
        "## combine df_test and its labels then throw away rows with -1 values\n",
        "df_test_labels_normalized = df_test_labels[df_test_labels['toxic']!=-1]\n",
        "df_test_normalized = df_test.set_index('id').join(df_test_labels_normalized.set_index('id'), how='right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7dRfir_2BRwX"
      },
      "outputs": [],
      "source": [
        "feature = ['comment_text']\n",
        "target = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "## convert into tf.data.Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((df_train[feature], df_train[target]))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((\n",
        "    df_test_normalized[feature],\n",
        "    df_test_normalized[target]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx1N9_iM2qFM"
      },
      "source": [
        "Use TextVectorization to convert text to sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "24-cUv-E_dW3"
      },
      "outputs": [],
      "source": [
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=50, ## max_tokens denotes number of words to be tokenized\n",
        "                               pad_to_max_tokens=True)\n",
        "\n",
        "vectorizer.adapt(train_data.map(lambda x, y: x).batch(2000)) ## use .map() to get the input only since in the dataset there are input and label\n",
        "\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfb95fRt2106"
      },
      "source": [
        "Load pretrained word embedding and put it into dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-UtzzDPBnWd",
        "outputId": "2f64894c-cf39-4cce-df18-e44cb71379f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = \"glove.6B.200d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEHcnkzE3HnU"
      },
      "source": [
        "Only use words from pretrained word embedding that exist in vectorizer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lYgicCpCVGN",
        "outputId": "b6c076ac-801a-4c15-9dd3-4b59f619550a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 18416 words (1584 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2  ## TextVectorization already includes OOV and padding, but pretrained glove file also includes OOV and padding so we add 2\n",
        "embedding_dim = 200  ## 200 as dimension comes from pretrained word embedding\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # Words not found in embedding index will be all-zeros.\n",
        "    # This includes the representation for \"padding\" and \"OOV\"\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    hits += 1\n",
        "  else:\n",
        "    misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6WV-ac53grD"
      },
      "source": [
        "Initialize embedding layer with 20000 of pretrained embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTqKXJUBGKaP"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfDShPDn3oQM"
      },
      "source": [
        "Create sequential model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73WC8yMbGcWH"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(50,), dtype='int64'),\n",
        "    embedding_layer,\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=False)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(6, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXW8hEMpJXBt"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform train data and test data from raw text to sequences"
      ],
      "metadata": {
        "id": "al4tS0vNstgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LsgcD8GvKkET"
      },
      "outputs": [],
      "source": [
        "## define function to convert raw text to sequences\n",
        "def to_sequence(x, y):\n",
        "  return vectorizer(x), y\n",
        "\n",
        "## batch, cache, and prefetch\n",
        "## set batch of 2000, since we have big vram and so we can better utilize the GPU\n",
        "## batch size tradeoff:\n",
        "## - big batch -> better GPU utilization -> faster training time -> lower accuracy\n",
        "## - small batch -> worse GPU utilization -> slower training time -> higher accuracy\n",
        "train_data = train_data.batch(32).map(to_sequence).cache().prefetch(tf.data.AUTOTUNE) ## batch_size of 32 is small and will result in slow training time but in our case gives better f1 score\n",
        "test_data = test_data.batch(20000).map(to_sequence).cache().prefetch(tf.data.AUTOTUNE)  ## for test data we can use big batch since we don't use it for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc2WKKyG4T4T"
      },
      "source": [
        "Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuF3BYvgHt_2"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "              optimizer='adam',\n",
        "              metrics=[tf.keras.metrics.Recall(thresholds=0.5), \n",
        "                       tf.keras.metrics.Precision(thresholds=0.5)])\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    epochs=3, \n",
        "                    validation_data=test_data,\n",
        "                    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2pTmXva4qL8"
      },
      "source": [
        "Find out f1 score of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ofTnH6cf2UYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276333dd-afeb-4fa2-a83a-501525c1efb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 3s 145ms/step\n",
            "4/4 [==============================] - 0s 137ms/step\n",
            "precision train:  0.8649898\n",
            "recall train:  0.67704713\n",
            "precision test:  0.63464135\n",
            "recall test:  0.68161124\n",
            "f1 train:  0.7595652663133241\n",
            "f1 test:  0.6572882447429294\n"
          ]
        }
      ],
      "source": [
        "## make prediction to all train and test dataset\n",
        "train_prediction = model.predict(train_data.map(lambda x, y: x).rebatch(20000)) ## rebatch to big batch size so prediction is much faster\n",
        "test_prediction = model.predict(test_data.map(lambda x, y: x))\n",
        "\n",
        "## define the metrics\n",
        "train_precision = tf.keras.metrics.Precision(thresholds=0.5)\n",
        "train_recall = tf.keras.metrics.Recall(thresholds=0.5)\n",
        "test_precision = tf.keras.metrics.Precision(thresholds=0.5)\n",
        "test_recall = tf.keras.metrics.Recall(thresholds=0.5)\n",
        "\n",
        "## prepare the label data\n",
        "# i don't know how to feed tf.data.Dataset directly into update_state() below,\n",
        "# so here i get only label from dataset -> unbatch the data -> force unpack them \n",
        "# into tensors with list(). This operation is slow!\n",
        "train_label = list(train_data.map(lambda x, y: y).unbatch())  ## get correct label from train data\n",
        "test_label = list(test_data.map(lambda x, y: y).unbatch())  ## get correct label from test data\n",
        "\n",
        "## calculate the metrics\n",
        "train_precision.update_state(train_label, train_prediction)\n",
        "train_recall.update_state(train_label, train_prediction)\n",
        "test_precision.update_state(test_label, test_prediction)\n",
        "test_recall.update_state(test_label, test_prediction)\n",
        "\n",
        "## get and turn result into numpy so we can do calculation\n",
        "train_precision = train_precision.result().numpy()\n",
        "train_recall = train_recall.result().numpy()\n",
        "test_precision = test_precision.result().numpy()\n",
        "test_recall = test_recall.result().numpy()\n",
        "\n",
        "## print the metric scores\n",
        "print('precision train: ', train_precision)\n",
        "print('recall train: ', train_recall)\n",
        "print('precision test: ', test_precision)\n",
        "print('recall test: ', test_recall)\n",
        "print('f1 train: ', 2 * train_precision * train_recall / (train_precision + train_recall))\n",
        "print('f1 test: ', 2 * test_precision * test_recall / (test_precision + test_recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM0eQw534cRD"
      },
      "source": [
        "Try predicting with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjC3uIYpCJMk"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=8, suppress=True)  ## print in decimal number, not scientific\n",
        "seq = vectorizer([['my nigga']])  ## turn text into sequence first using vectorizer\n",
        "model.predict(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOOfvR5s5jur"
      },
      "source": [
        "Append TextVectorization layer to the model, so we don't need to do separate preprocessing and can directly input raw text to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3omGinlDfcWM"
      },
      "outputs": [],
      "source": [
        "# Start by creating an explicit input layer. It needs to have a shape of  \n",
        "# (1,) (because we need to guarantee that there is exactly one string  \n",
        "# input per batch), and the dtype needs to be 'string'.\n",
        "end_to_end_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "    vectorizer,\n",
        "    model\n",
        "])\n",
        "\n",
        "end_to_end_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strGblAb54o9"
      },
      "source": [
        "Try the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN_hGj4Sf3N2"
      },
      "outputs": [],
      "source": [
        "end_to_end_model.predict([['my nigga']])  ## can directly input raw text, no need vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52dbGpER6AYj"
      },
      "source": [
        "Save the models with TextVectorization layer and without TextVectorization layer as saved_model format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n70NcdXEsHYb"
      },
      "outputs": [],
      "source": [
        "model.save('./saved_model/model')   ## model without TextVectorization layer\n",
        "end_to_end_model.save('./saved_model/end-to-end')  ## model with TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGThNNXe6L6F"
      },
      "source": [
        "Save to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GilutX-yxCUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8ee751-3cd2-4422-b2d0-8a203c377625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# ! cp -r saved_model gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1iJuEUI5rXr"
      },
      "source": [
        "Try to load the saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "C88pWOp1cC1T"
      },
      "outputs": [],
      "source": [
        "## load model without TextVectorization layer\n",
        "loaded_model = tf.keras.models.load_model('saved_model/model') \n",
        "\n",
        "## load model with TextVectorization layer\n",
        "loaded_end_to_end_model = tf.keras.models.load_model('saved_model/end-to-end')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOMNWzE56DL"
      },
      "source": [
        "Make prediction with loaded models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P30FMnw7c6U0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad564bca-4b5f-41e2-d5ed-8d2a2dbf4be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "loaded_model:  [[0.9763663  0.19614795 0.79277015 0.01046047 0.81799424 0.7410841 ]]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "loaded_end_to_end_model:  [[0.9763663  0.19614795 0.79277015 0.01046047 0.81799424 0.7410841 ]]\n"
          ]
        }
      ],
      "source": [
        "print('loaded_model: ', loaded_model.predict( vectorizer([['my nigga']]) )) ## need vectorizer\n",
        "print('loaded_end_to_end_model: ', loaded_end_to_end_model.predict( [['my nigga']] ))  ## no need vectorizer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}